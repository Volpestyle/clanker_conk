import { test } from "bun:test";
import assert from "node:assert/strict";
import { normalizeSettings, PERSONA_FLAVOR_MAX_CHARS } from "./settingsNormalization.ts";

test("normalizeSettings clamps and normalizes complex nested settings", () => {
  const normalized = normalizeSettings({
    botName: "x".repeat(120),
    botNameAliases: ["clank", "clank", "  ", "conk", "alias-".repeat(20)],
    llm: {
      provider: "XAI",
      model: "",
      temperature: 9,
      maxOutputTokens: 1
    },
    replyFollowupLlm: {
      enabled: true,
      provider: "not-real",
      model: "",
      maxToolSteps: 99,
      maxTotalToolCalls: -5,
      maxWebSearchCalls: 7,
      maxMemoryLookupCalls: -2,
      maxImageLookupCalls: 999,
      toolTimeoutMs: 999999
    },
    webSearch: {
      enabled: true,
      maxSearchesPerHour: 999,
      maxResults: 0,
      maxPagesToRead: 99,
      maxCharsPerPage: 80,
      safeSearch: false,
      providerOrder: ["serpapi", "serpapi", "brave", "unknown"],
      recencyDaysDefault: 0,
      maxConcurrentFetches: 99
    },
    videoContext: {
      enabled: true,
      maxLookupsPerHour: -1,
      maxVideosPerMessage: 99,
      maxTranscriptChars: 99_999,
      keyframeIntervalSeconds: -5,
      maxKeyframesPerVideo: 30,
      allowAsrFallback: true,
      maxAsrSeconds: 2
    },
    voice: {
      mode: "OPENAI_REALTIME",
      realtimeReplyStrategy: "NATIVE",
      generationLlm: {
        provider: "not-real",
        model: ""
      },
      thoughtEngine: {
        enabled: "yes",
        provider: "NOT-REAL",
        model: "",
        temperature: 9,
        eagerness: 999,
        minSilenceSeconds: 1,
        minSecondsBetweenThoughts: 9999
      },
      replyDecisionLlm: {
        provider: "CLAUDE-CODE",
        model: "",
        maxAttempts: 9,
        reasoningEffort: "HIGH"
      },
      openaiRealtime: {
        inputAudioFormat: "bad-format",
        outputAudioFormat: "g711_alaw"
      },
      geminiRealtime: {
        apiBaseUrl: "ftp://invalid.example/path",
        inputSampleRateHz: 0,
        outputSampleRateHz: 99_000
      },
      streamWatch: {
        minCommentaryIntervalSeconds: 1,
        maxFramesPerMinute: 9999,
        maxFrameBytes: 10,
        commentaryPath: "not-real",
        keyframeIntervalMs: 20,
        autonomousCommentaryEnabled: 0,
        brainContextEnabled: "yes",
        brainContextMinIntervalSeconds: -4,
        brainContextMaxEntries: 999,
        brainContextPrompt: `${"x".repeat(520)}   `
      },
      soundboard: {
        preferredSoundIds: ["first", "first", "second"]
      }
    },
    initiative: {
      allowedImageModels: "gpt-image-1.5, gpt-image-1.5, grok-imagine-image",
      allowedVideoModels: ["grok-imagine-video", "grok-imagine-video"],
      discovery: {
        rssFeeds: ["https://ok.example/feed", "not-a-url"],
        xHandles: ["@alice", "@alice", "bob"],
        redditSubreddits: ["r/memes", "memes"],
        xNitterBaseUrl: "https://nitter.example/path",
        sources: {
          reddit: false,
          x: true
        }
      }
    }
  });

  assert.equal(normalized.botName.length, 50);
  assert.deepEqual(normalized.botNameAliases, ["clank", "conk", "alias-alias-alias-alias-alias-alias-alias-alias-al"]);
  assert.equal(normalized.llm.provider, "xai");
  assert.equal(normalized.llm.model, "grok-3-mini-latest");
  assert.equal(normalized.llm.temperature, 2);
  assert.equal(normalized.llm.maxOutputTokens, 32);
  assert.equal(normalized.replyFollowupLlm.provider, "xai");
  assert.equal(normalized.replyFollowupLlm.model, "grok-3-mini-latest");
  assert.equal(normalized.replyFollowupLlm.maxToolSteps, 6);
  assert.equal(normalized.replyFollowupLlm.maxTotalToolCalls, 0);
  assert.equal(normalized.replyFollowupLlm.maxWebSearchCalls, 6);
  assert.equal(normalized.replyFollowupLlm.maxMemoryLookupCalls, 0);
  assert.equal(normalized.replyFollowupLlm.maxImageLookupCalls, 6);
  assert.equal(normalized.replyFollowupLlm.toolTimeoutMs, 60000);

  assert.equal(normalized.webSearch.maxSearchesPerHour, 120);
  assert.equal(normalized.webSearch.maxResults, 1);
  assert.equal(normalized.webSearch.maxPagesToRead, 5);
  assert.equal(normalized.webSearch.maxCharsPerPage, 350);
  assert.equal(normalized.webSearch.safeSearch, false);
  assert.equal(normalized.webSearch.recencyDaysDefault, 1);
  assert.equal(normalized.webSearch.maxConcurrentFetches, 10);
  assert.deepEqual(normalized.webSearch.providerOrder, ["serpapi", "brave"]);

  assert.equal(normalized.videoContext.maxLookupsPerHour, 0);
  assert.equal(normalized.videoContext.maxVideosPerMessage, 6);
  assert.equal(normalized.videoContext.maxTranscriptChars, 4000);
  assert.equal(normalized.videoContext.keyframeIntervalSeconds, 0);
  assert.equal(normalized.videoContext.maxKeyframesPerVideo, 8);
  assert.equal(normalized.videoContext.maxAsrSeconds, 15);

  assert.equal(normalized.voice.mode, "openai_realtime");
  assert.equal(normalized.voice.realtimeReplyStrategy, "native");
  assert.equal(normalized.voice.generationLlm.provider, "anthropic");
  assert.equal(normalized.voice.generationLlm.model, "claude-haiku-4-5");
  assert.equal(normalized.voice.thoughtEngine.enabled, true);
  assert.equal(normalized.voice.thoughtEngine.provider, "anthropic");
  assert.equal(normalized.voice.thoughtEngine.model, "claude-haiku-4-5");
  assert.equal(normalized.voice.thoughtEngine.temperature, 2);
  assert.equal(normalized.voice.thoughtEngine.eagerness, 100);
  assert.equal(normalized.voice.thoughtEngine.minSilenceSeconds, 8);
  assert.equal(normalized.voice.thoughtEngine.minSecondsBetweenThoughts, 600);
  assert.equal(normalized.voice.replyDecisionLlm.provider, "claude-code");
  assert.equal(normalized.voice.replyDecisionLlm.model, "sonnet");
  assert.equal(normalized.voice.replyDecisionLlm.enabled, true);
  assert.equal(normalized.voice.replyDecisionLlm.maxAttempts, 3);
  assert.equal(normalized.voice.replyDecisionLlm.reasoningEffort, "high");
  assert.equal(String(normalized.voice.replyDecisionLlm.prompts?.wakeVariantHint || "").length > 0, true);
  assert.equal(String(normalized.voice.replyDecisionLlm.prompts?.systemPromptStrict || "").length > 0, true);
  assert.equal(normalized.voice.openaiRealtime.inputAudioFormat, "pcm16");
  assert.equal(normalized.voice.openaiRealtime.outputAudioFormat, "pcm16");
  assert.equal(normalized.voice.geminiRealtime.apiBaseUrl, "https://generativelanguage.googleapis.com");
  assert.equal(normalized.voice.geminiRealtime.inputSampleRateHz, 8000);
  assert.equal(normalized.voice.geminiRealtime.outputSampleRateHz, 48000);
  assert.equal(normalized.voice.streamWatch.minCommentaryIntervalSeconds, 3);
  assert.equal(normalized.voice.streamWatch.maxFramesPerMinute, 600);
  assert.equal(normalized.voice.streamWatch.maxFrameBytes, 50_000);
  assert.equal(normalized.voice.streamWatch.commentaryPath, "auto");
  assert.equal(normalized.voice.streamWatch.keyframeIntervalMs, 250);
  assert.equal(normalized.voice.streamWatch.autonomousCommentaryEnabled, false);
  assert.equal(normalized.voice.streamWatch.brainContextEnabled, true);
  assert.equal(normalized.voice.streamWatch.brainContextMinIntervalSeconds, 1);
  assert.equal(normalized.voice.streamWatch.brainContextMaxEntries, 24);
  assert.equal(normalized.voice.streamWatch.brainContextPrompt.length, 420);
  assert.deepEqual(normalized.voice.soundboard.preferredSoundIds, ["first", "second"]);

  assert.deepEqual(normalized.initiative.allowedImageModels, ["gpt-image-1.5", "grok-imagine-image"]);
  assert.deepEqual(normalized.initiative.allowedVideoModels, ["grok-imagine-video"]);
  assert.deepEqual(normalized.initiative.discovery.rssFeeds, ["https://ok.example/feed"]);
  assert.deepEqual(normalized.initiative.discovery.xHandles, ["alice", "bob"]);
  assert.deepEqual(normalized.initiative.discovery.redditSubreddits, ["memes", "memes"]);
  assert.equal(normalized.initiative.discovery.xNitterBaseUrl, "https://nitter.example");
  assert.equal(normalized.initiative.discovery.sources.reddit, false);
  assert.equal(normalized.initiative.discovery.sources.x, true);
});

test("normalizeSettings handles memoryLlm defaults and discovery source fallbacks", () => {
  const normalized = normalizeSettings({
    memoryLlm: {},
    initiative: {
      discovery: {
        sources: {
          reddit: undefined,
          hackerNews: undefined,
          youtube: undefined,
          rss: undefined,
          x: undefined
        }
      }
    },
    prompt: {
      textGuidance: ["  one ", "one", "", "two"],
      voiceGuidance: [" alpha ", "alpha", "beta"],
      voiceOperationalGuidance: ["a", "a", "b"]
    }
  });

  assert.equal(normalized.memoryLlm.provider, "anthropic");
  assert.equal(normalized.memoryLlm.model, "claude-haiku-4-5");
  assert.deepEqual(normalized.prompt.textGuidance, ["one", "two"]);
  assert.deepEqual(normalized.prompt.voiceGuidance, ["alpha", "beta"]);
  assert.deepEqual(normalized.prompt.voiceOperationalGuidance, ["a", "b"]);

  assert.equal(typeof normalized.initiative.discovery.sources.reddit, "boolean");
  assert.equal(typeof normalized.initiative.discovery.sources.hackerNews, "boolean");
  assert.equal(typeof normalized.initiative.discovery.sources.youtube, "boolean");
  assert.equal(typeof normalized.initiative.discovery.sources.rss, "boolean");
  assert.equal(typeof normalized.initiative.discovery.sources.x, "boolean");
});

test("normalizeSettings uses provider-appropriate memoryLlm model fallback", () => {
  const normalized = normalizeSettings({
    memoryLlm: {
      provider: "openai",
      model: ""
    }
  });

  assert.equal(normalized.memoryLlm.provider, "openai");
  assert.equal(normalized.memoryLlm.model, "claude-haiku-4-5");
});

test("normalizeSettings keeps stt pipeline voice generation and reply decider independent from main llm", () => {
  const normalized = normalizeSettings({
    llm: {
      provider: "claude-code",
      model: "opus"
    },
    voice: {
      mode: "stt_pipeline",
      generationLlm: {
        provider: "openai",
        model: "claude-haiku-4-5"
      },
      replyDecisionLlm: {
        provider: "openai",
        model: "claude-haiku-4-5",
        maxAttempts: 2,
        reasoningEffort: "not-real"
      }
    }
  });

  assert.equal(normalized.voice.generationLlm.provider, "openai");
  assert.equal(normalized.voice.generationLlm.model, "claude-haiku-4-5");
  assert.equal(normalized.voice.replyDecisionLlm.provider, "openai");
  assert.equal(normalized.voice.replyDecisionLlm.model, "claude-haiku-4-5");
  assert.equal(normalized.voice.replyDecisionLlm.enabled, true);
  assert.equal(normalized.voice.replyDecisionLlm.maxAttempts, 2);
  assert.equal(normalized.voice.replyDecisionLlm.reasoningEffort, "minimal");
  assert.equal(normalized.voice.realtimeReplyStrategy, "brain");
});

test("normalizeSettings preserves custom voice decider prompt overrides", () => {
  const normalized = normalizeSettings({
    voice: {
      replyDecisionLlm: {
        prompts: {
          wakeVariantHint: "custom wake rule for {{botName}}",
          systemPromptCompact: "compact {{botName}}",
          systemPromptFull: "full {{botName}}",
          systemPromptStrict: "strict {{botName}}"
        }
      }
    }
  });

  assert.equal(normalized.voice.replyDecisionLlm.prompts.wakeVariantHint, "custom wake rule for {{botName}}");
  assert.equal(normalized.voice.replyDecisionLlm.prompts.systemPromptCompact, "compact {{botName}}");
  assert.equal(normalized.voice.replyDecisionLlm.prompts.systemPromptFull, "full {{botName}}");
  assert.equal(normalized.voice.replyDecisionLlm.prompts.systemPromptStrict, "strict {{botName}}");
});

test("normalizeSettings preserves long media prompt craft guidance blocks", () => {
  const longGuidance = `line one\n${"x".repeat(1200)}\nline three`;
  const normalized = normalizeSettings({
    prompt: {
      mediaPromptCraftGuidance: longGuidance
    }
  });

  assert.equal(normalized.prompt.mediaPromptCraftGuidance, longGuidance);
});

test("normalizeSettings allows longer persona flavor values", () => {
  const withinLimit = "x".repeat(PERSONA_FLAVOR_MAX_CHARS);
  const normalizedWithinLimit = normalizeSettings({
    persona: {
      flavor: withinLimit
    }
  });
  assert.equal(normalizedWithinLimit.persona.flavor, withinLimit);

  const overLimit = `${"y".repeat(PERSONA_FLAVOR_MAX_CHARS)}overflow`;
  const normalizedOverLimit = normalizeSettings({
    persona: {
      flavor: overLimit
    }
  });
  assert.equal(normalizedOverLimit.persona.flavor.length, PERSONA_FLAVOR_MAX_CHARS);
});
